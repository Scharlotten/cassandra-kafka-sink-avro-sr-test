The docker compose file should hold the infrastructure needed to configure the connector. 

If you want to re-create the test scenario make sure that you update the volume mappings

First:
Look for this section in the `docker-compose.yaml` file
```
    volumes:
      - ./jars:/etc/kafka-connect/jars
      - ./conn-config:/workarea
```
Make sure you create a `conn-cofnig` directory that holds the `conn-config-template` files. 
Populate the files with the relevant API keys and secrets to authenticate to the Kafka cluster. 

See this section from the worker:


value.converter.schema.registry.url=<SCHEMA_REGISTRY_URL>
value.converter.enhanced.avro.schema.support=true
value.converter.schema.registry.basic.auth.credentials.source=USER_INFO
value.converter.schema.registry.basic.auth.user.info=<CONFLUENT_SCHEMA_REGITRY_API_KEY:SCHEMA_REGSITRY_SECRET>

sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
username="<KAFKA_BOOTSTRAP_API_KEY>" password="<KAFKA_API_SECRET>";
security.protocol=SASL_SSL

consumer.ssl.endpoint.identification.algorithm=https
consumer.sasl.mechanism=PLAIN
consumer.request.timeout.ms=20000
consumer.retry.backoff.ms=500
consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
username="<KAFKA_BOOTSTRAP_API_KEY>" password="<KAFKA_API_SECRET>";
consumer.security.protocol=SASL_SSL


confluent.topic.bootstrap.servers=pkc-619z3.us-east1.gcp.confluent.cloud:9092
confluent.topic.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule \
required username="<KAFKA_BOOTSTRAP_API_KEY>" password="<KAFKA_API_SECRET>";
confluent.topic.security.protocol=SASL_SSL
confluent.topic.sasl.mechanism=PLAIN

Once configured - make sure you mount the volume to the connect container with the files that have the values configured. 

Download the connector from Github. 
