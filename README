First register for a Confluent Cloud account. 

Create a cluster in any cloud provider in Confluent Cloud. 

Create a topic called "datagen-topic". 

Once the topic is ready, create a sample-data connector that produces **avro** messages into the topic.

Create and API key and secret to access the cluster (this can belong to your account no need to create a service account).

Save the API key and secret as it will be needed to authenticate to the Kafka cluster. 

![alt text](image.png) 

Select the Stocks (from the UI) 
(It is important you select the stocks, as the keyspace and the table structure will need to be pre-created, you can choose any sample data from here, just make sure you pre-create the table, and you configure the mapping in the cassandra-conn.properties file)

The docker compose file desdcribes all the infrastructure components needed to configure the connector. 

If you want to re-create the test scenario make sure that you update the volume mappings in the docker-compose.yaml file. 

First:
Look for this section in the `docker-compose.yaml` file
```
    volumes:
      - ./jars:/etc/kafka-connect/jars
      - ./conn-config:/workarea
```
Make sure you create a `conn-cofnig` directory that copies all the `conn-config-template` files. 
Populate the files with the relevant API keys and secrets to authenticate to the Kafka cluster. 

See this section from the worker:

value.converter.schema.registry.url=<SCHEMA_REGISTRY_URL>
value.converter.enhanced.avro.schema.support=true
value.converter.schema.registry.basic.auth.credentials.source=USER_INFO
value.converter.schema.registry.basic.auth.user.info=<CONFLUENT_SCHEMA_REGITRY_API_KEY:SCHEMA_REGSITRY_SECRET>

sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
username="<KAFKA_BOOTSTRAP_API_KEY>" password="<KAFKA_API_SECRET>";
security.protocol=SASL_SSL

consumer.ssl.endpoint.identification.algorithm=https
consumer.sasl.mechanism=PLAIN
consumer.request.timeout.ms=20000
consumer.retry.backoff.ms=500
consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
username="<KAFKA_BOOTSTRAP_API_KEY>" password="<KAFKA_API_SECRET>";
consumer.security.protocol=SASL_SSL


confluent.topic.bootstrap.servers=pkc-619z3.us-east1.gcp.confluent.cloud:9092
confluent.topic.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule \
required username="<KAFKA_BOOTSTRAP_API_KEY>" password="<KAFKA_API_SECRET>";
confluent.topic.security.protocol=SASL_SSL
confluent.topic.sasl.mechanism=PLAIN

Once configured - make sure you mount the volume to the connect container with the files that have the values configured. 

Download the connector from Github. 
https://github.com/datastax/kafka-sink/releases/tag/1.7.2

Create a directory called "jars" 

Move the connector jar to this directory. 


Once everything is configured, do 

`docker compose up -d`

Follow the connector logs 
`docker compose logs connect -f `

Enter into the cassandra pod to check if data is coming through

`docker exec -it kafka-test-cassandra-1 cqlsh`
`select * from kafka_sink.datagen limit 10;`

There wont' be too many rows in the table as by the key the data gets updated and only 7 keys are present in the dummy data. 

